#########################################################
Biobjective Performance Assessment with the Coco Platform
#########################################################

.. |coco_problem_t| replace:: 
  ``coco_problem_t``
.. _coco_problem_t: 
  http://numbbo.github.io/coco-doc/C/coco_8h.html#a408ba01b98c78bf5be3df36562d99478



This document details the specificities when assessing the performance of numerical black-box optimizers
on multi-objective problems within the Coco platform and in particular on the biobjective test suite
``bbob-biobj``, described in more detail in [bbob-biobj-functions-doc]_ .

Opposed to the single-objective ``bbob`` test suite, the biobjective ``bbob-biobj`` test suite does
not provide analytical forms of its optima, i.e. the Pareto set/Pareto front except for one of
its 55 functions. The performance assessment therefore has to be relative to the best known
approximations and this document details how this is implemented.


Contents:

.. toctree::
   :maxdepth: 2

   
   


.. todo::   * perf assessement is relative - we face a problem: we do not have the optimum.
			* How do we deal with this problem? [ this should probably be a section]
				* estimate the optimum
				* but approximation, meant to change / be improved - therefore need to ensure compatibility
				* compatibility + easy re-estimation of the performance when the reference set is improved	
			* we do not have the optimum (except for f1)
 			* we estimate it (how: running some algorithms) and it is meant to be changed with time (improved with time)
 			* things are based on the archive of nondominated solutions
 			* we measure the hypervolume difference between the dynamic archive and this reference set.
			* negative hyp-vol diff values are expected (means the algorithm improves over the current reference set)
			* archive is improved over time, whenever we have a new point entering the archive we recompute and log the hyp-vol difference.

			
Dealing with Unknown Optima
===========================
The equivalent of a global optimum in the multi-objective case is the set of Pareto-optimal
or efficient solutions, also known as Pareto set. If we assume the search space to be
:math:`\mathbb{R}^n` and the minimization of two objective
functions :math:`f_1: x\in \mathbb{R}^n \mapsto f_1(x)\in\mathbb{R}` and :math:`f_1: x\in \mathbb{R}^n \mapsto f_1(x)\in\mathbb{R}`,
a solution :math:`x\in\mathbb{R}^n` is called Pareto-optimal if it is not dominated
by any other solution :math:`y\in\mathbb{R}^n` or, in other words, if

.. math::
  
  \not\exists y \text{ s.t. } (f_1(y)< f_1(x) \text{ and } f_2(y)\leq f_2(x)) \text{ or } (f_2(y)\leq f_2(x) \text{ and } f_2(y)< f_2(x)).

The image of the Pareto set under the vector-valued objective function
:math:`f(x)= (f_1(x), f_2(x))` is called Pareto front.

When combining single-objective functions to multi-objective ones as in the case of the ``bbob-biobj``
suite, one cannot expect that Pareto set and Pareto front can be described in analytical form---even
if the single-objective optima are known. Comparing algorithm performance can therefore only be
done relatively to the best known optimum. In the multi-objective
case, where with the Pareto set a set of solutions is sought, we call this approximation
**reference set**. In practice, such a reference set is typically generated by running a certain set
of algorithms on the considered problem ahead of the performance assessment.

This has two main implications:

* Performance can only be judged relatively to the reference set. The better the algorithms
  used to create the reference set have been, the more accurate the performance assessment.

* The reference set is expected to evolve over time, in terms of becoming a better and better
  approximation of the actual Pareto set/Pareto front if more and more algorithms are
  compared.

The performance assessment via the Coco platform addresses both issues, see
`Choice of Reference Set and Target Difficulties`_ and
`Data storage and Future Recalculations of Indicator Values`_ below for details.
Before we discuss these issues, however, let us have a look on the actual performance
criterion used for the ``bbob-biobj`` test suite, assuming that a reference set is given.



Bounded vs. Unbounded Domain
============================
All bi-objective functions, provided in the ``bbob-biobj`` suite are unbounded, i.e., defined
on the entire real-valued space :math:`\mathbb{R}^n` with :math:`n` the search space dimension.
Nevertheless, the implementation in the Coco platform as |coco_problem_t| allows the optimizer
to retrieve a **search domain of interest** to get an idea about where reasonable intial
search points shall lie. Note that, due to the nature of the ``bbob-biobj`` function definitions,
however, there is no guarantee that also the entire Pareto set lies within this search domain of
interest---it is only guaranteed that the extremal solutions of the Pareto set lie within this region.



Biobjective Performance Assessment in Coco: A Set-Indicator Value Replaces the Objective Function
=================================================================================================
The general concepts of how the Coco platform suggests to benchmark multi-objective algorithms
is the same than in the single-objective case: for each optimization algorithm, we record the
(expected) runtimes to reach given target precisions for each problem in a given benchmark suite.
A problem thereby consists of a (vector-valued) objective function, its search space dimension,
and a concrete instantiation of it (see [coco-functions-doc]_ ). For defining the target precision
of such a problem, we assume a single-objective criterion which is to be optimized. In the single-objective
case, this is the objective function, in the case of the ``bbob-biobj`` test suite, 
a so-called quality indicator can transform the multi-objective problem into a single-objective
one.

In particular, we suggest to (mainly) use the hypervolume indicator of the archive of all non-dominated
solutions evaluated so far as the quality of an algorithm but principally, other quality indicators
of the archive can be used as well.





.. figure:: pics/IHDoutside.*
   :align: center
   :width: 60%

   Illustration of Coco's performance criterion in the bi-objective case if no solution of the
   archive (blue filled circles) dominates the nadir point (black filled circle), i.e., the
   hypervolume of the reference set (aka the best known Pareto front approximation, red triangles)
   plus the shortest distance of an archive member to the region of interest (ROI), delimited by
   the nadir point. Here, it is the forth point from the left that defines the smallest distance.
   
.. figure:: pics/IHDinside.*
   :align: center
   :width: 60%

   Illustration of Coco's performance criterion in the bi-objective case if the nadir point
   (black filled circle) is dominated by a solution in the archive (blue filled circles):
   the difference between the hypervolume of the reference
   set (aka Pareto front approximation, red triangles) and the hypervolume of the archive is given
   as the size of the two blue shaded areas minus the size of the green area.




Specificities for the ``bbob-biobj`` performance criterion

* algorithm performance = quality of archive of non-dominated solutions found so far

* normalization of objective space before indicator calculation such that the
  region of interest (ROI) :math:`[z_{\text{ideal}}, z_{\text{nadir}}]`, defined by
  the ideal and nadir point is mapped to :math:`[0, 1]^2`

* if nadir point is dominated by a point in the archive: quality = hypervolume of archive wrt nadir point
  as hypervolume reference point

* if nadir point is not dominated by archive: quality = negative distance of archive to the ROI

* what is of actual interest is the quality indicator difference to the reference set

Implications on the performance criterion:

* As the reference set approaches the Pareto set, the optimal quality indicator difference goes to 0`

* Because the reference set is always a finite approximation of the Pareto set, negative quality
  indicator differences can occur.

* Because the quality of the archive is used as performance criterion, no population size has to be
  prescribed to the algorithm. In particular, steady-state and generational algorithms can be 
  compared directly as well as algorithms with varying population size and algorithms which carry
  along their external archive themselves.
  
---

* why hypervolume (can also be in principle with other indicators)

* Evaluation based on the complete archive of nondominated solutions, independent of population size (Tobias)

* explain - give formula for the computation of the hypervolume (if there are no points dominating the Nadir)



Choice of Reference Set and Target Difficulties
===============================================
Choice of the targets based on best estimation of Pareto front (using all the data we have) - chosen instance wise

relative targets (in terms of the hypervolume difference to the hypervolume of the reference set)
are chosen the same for all functions, dimensions, and instances: recorded are 100 targets per order of magnitude,
equi-distantly chosen on the log-scale.


Displayed are finally only 10 targets per order of magnitude, in total
51 of them between :math:`10^0` and :math:`10^{-5}`

Note that due to the approximative nature of the reference set and its hypervolume, negative hypervolume values are
possible. The Coco platform stores all

Remind that performance assessment is "relative" because best
estimation of the front is meant to change. Hence ECDF plots are meant
to be reploted.



Data storage and Future Recalculations of Indicator Values
==========================================================
Having a good approximation of the Pareto set/Pareto front is crucial in accessing
algorithm performance with the above suggested performance criterion. In order to allow
the reference set to approximate the Pareto set/Pareto front better and better over time,
the Coco platform records every non-dominated solution over the algorithm run.
Algorithm data sets, submitted through the Coco platform's web page, can therefore
be used to improve the quality of the reference set by adding all solutions to the
reference set which are non-dominated to it. 

Recording every new non-dominated solution within every algorithm run also allows to
recover the algorithm runs after the experiment and to recalculate the corresponding
hypervolume difference values if the reference set changes in the future.




Instances and Generalization Experiment
=======================================
* we record for 10 instances but display result for only 5. This will allow us to generate data for an unbiased
  generalization test on the unseen instances

  
  

Acknowledgements
================
This work was supported by the grant ANR-12-MONU-0009 (NumBBO) 
of the French National Research Agency.
  
   

.. ############################# References #########################################
.. raw:: html
    
    <H2>References</H2>

   
.. [bbob-biobj-functions-doc] The BBOBies. **Function Documentation of the bbob-biobj Test Suite**. http://numbbo.github.io/coco-doc/bbob-biobj/functions/

.. [coco-functions-doc] The BBOBies. **COCO: Performance Assessment**. http://numbbo.github.io/coco-doc/perf-assessment/

.. [coco-doc] The BBOBies. **COCO: A platform for Comparing Continuous Optimizers in a Black-Box Setting**. http://numbbo.github.io/coco-doc/